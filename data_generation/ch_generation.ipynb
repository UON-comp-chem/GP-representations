{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Insert system path to use scripts in gplearn\n",
    "sys.path.append(\"/home/xinyu/WSL-workspace/GP-representations/CatHub\")\n",
    "sys.path.append(\"/home/xinyu/WSL-workspace/GP-representations/\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ase.db import connect\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from gplearn.mongo import make_doc_from_atoms, make_atoms_from_doc\n",
    "from gplearn.atoms_operators import fingerprint_adslab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.system(\"ase db postgresql://catvisitor:eFjohbnD57WLYAJX@catalysishub.c8gwuc8jwb7l.us-west-2.rds.amazonaws.com:5432/catalysishub pub_id=MamunHighT2019,relaxed=0,adsorbate=H --insert-into {}\".format(str(UNRELAXED_ASE_DB_PATH)))\\nos.system(\"ase db postgresql://catvisitor:eFjohbnD57WLYAJX@catalysishub.c8gwuc8jwb7l.us-west-2.rds.amazonaws.com:5432/catalysishub pub_id=MamunHighT2019,relaxed=1,adsorbate=H --insert-into {}\".format(str(RELAXED_ASE_DB_PATH)))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "RELAXED_ASE_DB_PATH = Path(\"ch-relaxed.db\")\n",
    "UNRELAXED_ASE_DB_PATH = Path(\"ch-unrelaxed.db\")\n",
    "PROCESSED_DATA_PATH = 'scidata-original-ch-0411.pkl'\n",
    "ADSORBATE= \"CH\"\n",
    "# RELAXED_ASE_DB and UNRELAXED_ASE_DB could be generated using the following commented code\n",
    "\"\"\"\n",
    "os.system(\"ase db postgresql://catvisitor:eFjohbnD57WLYAJX@catalysishub.c8gwuc8jwb7l.us-west-2.rds.amazonaws.com:5432/catalysishub pub_id=MamunHighT2019,relaxed=0,adsorbate=H --insert-into {}\".format(str(UNRELAXED_ASE_DB_PATH)))\n",
    "os.system(\"ase db postgresql://catvisitor:eFjohbnD57WLYAJX@catalysishub.c8gwuc8jwb7l.us-west-2.rds.amazonaws.com:5432/catalysishub pub_id=MamunHighT2019,relaxed=1,adsorbate=H --insert-into {}\".format(str(RELAXED_ASE_DB_PATH)))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CH4(g)-1.5H2(g) + * -> CH*'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from cathub.cathubsql import CathubSQL\n",
    "\n",
    "db = CathubSQL()\n",
    "\n",
    "dataframe = db.get_dataframe(pub_id='MamunHighT2019',\n",
    "                             include_atoms=False,\n",
    "                             reactants=['CH4gas', 'H2gas'],\n",
    "                             products=['CHstar']\n",
    "                             )\n",
    "dataframe.to_csv(\"ch.csv\")\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"ch.csv\")\n",
    "np.unique(df.equation) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3536\n"
     ]
    }
   ],
   "source": [
    "adsorption_dict = {}\n",
    "launchdir_dict = {}\n",
    "launchdir_2_relax_idx = {}\n",
    "with connect(RELAXED_ASE_DB_PATH) as conn:\n",
    "    print(conn.count())\n",
    "    for i in range(conn.count()):\n",
    "        atmsrw = conn.get(1+i)\n",
    "        if \"adsorbate\" in dir(atmsrw):\n",
    "            adsorption_dict[atmsrw.unique_id] = atmsrw.toatoms()\n",
    "            launchdir_dict[atmsrw.unique_id] = atmsrw.launch_dir\n",
    "            launchdir_2_relax_idx[atmsrw.launch_dir] = 1+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3506\n"
     ]
    }
   ],
   "source": [
    "launchdir_2_unrelax_idx = {}\n",
    "with connect(UNRELAXED_ASE_DB_PATH) as conn:\n",
    "    print(conn.count())\n",
    "    for i in range(conn.count()):\n",
    "        atmsrw = conn.get(1+i)\n",
    "        launchdir_2_unrelax_idx[atmsrw.launch_dir] = 1+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaspy.mongo import make_doc_from_atoms, make_atoms_from_doc\n",
    "from gplearn.atoms_operators import fingerprint_adslab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [01:53,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 did not get the unrelaxed geometry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "835it [05:48,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834 did not get the unrelaxed geometry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1508it [10:31,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  samples were discarded because they are duplicated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "reserved_ase_ids = []\n",
    "discard_duplicated = []\n",
    "for record in tqdm(df.iterrows()):\n",
    "    atoms_id = record[1]['atoms_id']\n",
    "    atoms_id = json.loads(str(atoms_id).replace('\\'', '\"'))\n",
    "    match = False\n",
    "    for idx in atoms_id:\n",
    "        if idx in adsorption_dict.keys():\n",
    "            relaxed_id = idx\n",
    "            match = True\n",
    "            break\n",
    "    if match == False:\n",
    "        print(record[0], \" did not find relaxed structures\")\n",
    "        continue\n",
    "    if relaxed_id in reserved_ase_ids:\n",
    "        discard_duplicated.append(record[0])\n",
    "        continue\n",
    "    else:\n",
    "        reserved_ase_ids.append(relaxed_id)\n",
    "    relaxed_atoms = adsorption_dict[relaxed_id]\n",
    "    launch_dir = launchdir_dict[relaxed_id]\n",
    "    \n",
    "    pos = relaxed_atoms.positions\n",
    "    _sortidx = np.argsort(pos[:, -1][:12])\n",
    "    sortidx = np.arange(len(pos))\n",
    "    sortidx[:12] = _sortidx\n",
    "    relaxed_atoms = relaxed_atoms[sortidx]\n",
    "    with connect(RELAXED_ASE_DB_PATH) as conn:\n",
    "        relaxed_atmsrw = conn.get(launchdir_2_relax_idx[launch_dir])\n",
    "    adsorbate = relaxed_atmsrw.adsorbate\n",
    "    energy = record[1]['reaction_energy']\n",
    "    \"\"\"\n",
    "    if record[1]['equation'] == '0.5H2(g) + * -> H*':\n",
    "        energy = record[1]['reaction_energy']\n",
    "    elif record[1]['equation'] == 'H2(g) + 2* -> 2H*':\n",
    "        energy = record[1]['reaction_energy']/2\n",
    "    \"\"\"\n",
    "    metalB = \"nan\"\n",
    "    if 'metalB' in dir(relaxed_atmsrw):\n",
    "        metalB = relaxed_atmsrw.metalB\n",
    "\n",
    "    tags = np.zeros(len(relaxed_atoms)).astype(int)\n",
    "    tags[12:] = 1\n",
    "    tags = tags.tolist()\n",
    "    relaxed_atoms.set_tags(tags)\n",
    "    doc = make_doc_from_atoms(relaxed_atoms,\n",
    "                              unique_id = relaxed_atmsrw.unique_id,\n",
    "                              slab_name = relaxed_atmsrw.slab_name,\n",
    "                              reconstructed = relaxed_atmsrw.reconstructed, \n",
    "                              site = relaxed_atmsrw.site,\n",
    "                              metalA = relaxed_atmsrw.metalA,\n",
    "                              metalB = metalB,\n",
    "                              site_type = relaxed_atmsrw.site_type,\n",
    "                              fw_id = relaxed_atmsrw.fw_id,\n",
    "                              energy = energy,\n",
    "                              adsorbate = adsorbate)\n",
    "    ffp = fingerprint_adslab(relaxed_atoms, adsorbate = ADSORBATE[0])\n",
    "    for name, value in ffp.items():\n",
    "        doc[name] = value\n",
    "        \n",
    "    if launch_dir in launchdir_2_unrelax_idx.keys():\n",
    "        unrealxed_id = launchdir_2_unrelax_idx[launch_dir]\n",
    "        with connect(UNRELAXED_ASE_DB_PATH) as conn:\n",
    "            unrelaxed_atmsrw = conn.get(unrealxed_id)\n",
    "            unrelaxed_atoms = conn.get(unrealxed_id).toatoms()\n",
    "        unrelaxed_atoms = unrelaxed_atoms[sortidx]\n",
    "        unrelaxed_atoms.set_tags(tags)\n",
    "        un_doc = make_doc_from_atoms(unrelaxed_atoms, \n",
    "                                     reconstructed = unrelaxed_atmsrw.reconstructed, \n",
    "                                     fw_id = unrelaxed_atmsrw.fw_id,\n",
    "                                     site = unrelaxed_atmsrw.site,\n",
    "                                     site_type = unrelaxed_atmsrw.site_type,\n",
    "                                    unique_id = unrelaxed_atmsrw.unique_id,)\n",
    "        doc['initial_configuration'] = un_doc\n",
    "        ifp= fingerprint_adslab(unrelaxed_atoms, adsorbate =ADSORBATE[0])\n",
    "        for name, value in ifp.items():\n",
    "            doc['initial_configuration'][name] = value\n",
    "    else:\n",
    "        print(\"{} did not get the unrelaxed geometry\".format(record[0]))    \n",
    "    docs.append(doc)\n",
    "print(len(discard_duplicated), \" samples were discarded because they are duplicated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  samples were discarded because they are duplicated.\n"
     ]
    }
   ],
   "source": [
    "print(len(discard_duplicated), \" samples were discarded because they are duplicated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../scidata-original-ch-0403.pkl\", 'rb') as fp:\n",
    "    docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    if 'initial_configuration' in doc.keys():\n",
    "        assert doc['fw_id'] == doc['initial_configuration']['fw_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 1508 optmized samples, 1506 have initial strucutres\n"
     ]
    }
   ],
   "source": [
    "unopt = [1 for n in docs if 'initial_configuration' in n.keys()]\n",
    "print(\"Get {} optmized samples, {} have initial strucutres\".format(len(docs), len(unopt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-H bond broken, filter out\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as bridge-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as bridge-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "C-H bond broken, filter out\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "C-H bond broken, filter out\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as bridge-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as top-tilt\n",
      "C-H bond broken, filter out\n",
      "Warning: A strong site match could not be found!\n",
      "  structure labeled as hollow-tilt\n"
     ]
    }
   ],
   "source": [
    "from gplearn.geometry_rebuilt import SiteClassification, SiteLabel2AB\n",
    "from gplearn.mongo import make_atoms_from_doc\n",
    "from ase.geometry import find_mic\n",
    "dist = []\n",
    "atoms_list = []\n",
    "new_docs = []\n",
    "for idx, doc1 in enumerate(docs):\n",
    "    doc = deepcopy(doc1)\n",
    "    atoms = make_atoms_from_doc(doc)\n",
    "    pos = atoms.positions\n",
    "    d = find_mic([pos[-1] - pos[-2]], atoms.cell)[1]\n",
    "    if d > 1.20:\n",
    "        print(\"C-H bond broken, filter out\")\n",
    "        continue\n",
    "    atoms = make_atoms_from_doc(doc)\n",
    "    if 'initial_configuration' in doc.keys():\n",
    "        iatoms = make_atoms_from_doc(doc['initial_configuration'])\n",
    "    else:\n",
    "        iatoms = atoms.copy()\n",
    "    sc = SiteClassification(atoms[:13], A = iatoms[:13])\n",
    "    metal_B = doc['metalB']\n",
    "    if metal_B == 'nan':\n",
    "        metal_B = None\n",
    "    sclabel = SiteLabel2AB(sc.get_site()[1], doc['metalA'], B = metal_B)\n",
    "    if doc['site_type']  != sclabel:\n",
    "        print(idx,  \" site inconsistent, doc is {}, get {}, updated\".format(doc['site_type'], sclabel))\n",
    "        doc['site_type'] = sclabel\n",
    "    new_docs.append(doc)\n",
    "docs = new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now build the rebuilt data set\n",
    "from gplearn.defaults import elements_list\n",
    "from gplearn.geometry_rebuilt import SiteRebuild\n",
    "A1 = elements_list\n",
    "slabnames = np.array([doc['slab_name'] for doc in docs])\n",
    "A1 = [n for n in np.unique(slabnames) if n in A1]\n",
    "L12 = [n for n in np.unique(slabnames) if '3' in n]\n",
    "L10 = [n for n in np.unique(slabnames) if n not in A1+L12]\n",
    "slab_type_dict = {n: 'A1' for n in A1}\n",
    "for i in L12:\n",
    "    slab_type_dict[i] = 'L12'\n",
    "for i in L10:\n",
    "    slab_type_dict[i] = 'L10'\n",
    "site_dict = {\"A1\": [\"A\", \"A_A_A|HCP\", \"A_A|A\", \"A_A_A|FCC\"],\n",
    "             \"L10\":[\"A_B|B\", \"B_B|A\", \"A_A|B\", \"A_B_B|HCP\", \"A\", \"B\", \"A_A_B|FCC\", \\\n",
    "                    \"A_B_B|FCC\", \"A_A_B|HCP\", \"A_B|A\"],\n",
    "             \"L12\":[\"A_A_A|FCC\", \"A_A|A\",\"A_B|A\",\"A_A|B\",\n",
    "                       \"A\",\"B\",\"A_A_B|FCC\",\"A_A_A|HCP\",\"A_A_B|HCP\"]}\n",
    "special_site = {\"A_A|A\": \"A_A|B\", \"B_B|B\": \"B_B|A\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:00, 90.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 79 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "814it [00:11, 70.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 801 since it is a strange A_A_B_B site.\n",
      "Cannot get the rebuilt geometry of 808 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1072it [00:15, 67.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 1059 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1235it [00:17, 67.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 1226 since it is a strange A_A_B_B site.\n",
      "Cannot get the rebuilt geometry of 1233 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1278it [00:18, 66.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 1265 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1300it [00:18, 67.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 1288 since it is a strange A_A_B_B site.\n",
      "Cannot get the rebuilt geometry of 1291 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1343it [00:19, 68.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get the rebuilt geometry of 1331 since it is a strange A_A_B_B site.\n",
      "Cannot get the rebuilt geometry of 1335 since it is a strange A_A_B_B site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1504it [00:21, 69.51it/s]\n"
     ]
    }
   ],
   "source": [
    "for _idx, doc in tqdm(enumerate(docs)):\n",
    "    if \"initial_configuration\" not in doc.keys():\n",
    "        continue\n",
    "    slab =doc['slab_name']\n",
    "    iatoms = make_atoms_from_doc(doc['initial_configuration'])\n",
    "    initial_sites = site_dict[slab_type_dict[slab]]\n",
    "    if doc['metalB'] == 'nan':\n",
    "        metalB = None\n",
    "    else:\n",
    "        metalB = doc['metalB']\n",
    "    sc = SiteRebuild(iatoms, slab_type_dict[slab], metalA =doc['metalA'], metalB =metalB,  adsorbate=ADSORBATE)\n",
    "    site_type = doc['site_type']\n",
    "    if site_type not in initial_sites:\n",
    "        if site_type in special_site.keys():\n",
    "            site_type = special_site[site_type]\n",
    "        else:\n",
    "            print(\"Cannot get the rebuilt geometry of {} since it is a strange {} site.\".format(_idx, doc['site_type']))\n",
    "            continue\n",
    "    rebuilt = sc.rebuild_initial(site_type)\n",
    "    rebuilt_doc = make_doc_from_atoms(rebuilt,\n",
    "                                      site_type = site_type)\n",
    "    docs[_idx]['rebuilt_configuration'] = rebuilt_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 1504 optmized samples, 1502 have initial strucutres, 1491 have rebuilt structures\n"
     ]
    }
   ],
   "source": [
    "unopt = [1 for n in docs if 'initial_configuration' in n.keys()]\n",
    "rebuilt = [1 for n in docs if 'rebuilt_configuration' in n.keys()]\n",
    "print(\"Get {} optmized samples, {} have initial strucutres, {} have rebuilt structures\".format(len(docs), len(unopt), len(rebuilt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = [make_atoms_from_doc(d) for d in docs]\n",
    "assert len(np.unique([n[12].symbol for n in atoms])) == 1\n",
    "atoms = [make_atoms_from_doc(d['initial_configuration']) for d in docs if 'initial_configuration' in d.keys()]\n",
    "assert len(np.unique([n[12].symbol for n in atoms])) == 1\n",
    "atoms = [make_atoms_from_doc(d['rebuilt_configuration']) for d in docs if 'rebuilt_configuration' in d.keys()]\n",
    "assert len(np.unique([n[12].symbol for n in atoms])) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(PROCESSED_DATA_PATH, 'wb') as fp:\n",
    "    pickle.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O'], dtype='<U1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([n[12].symbol for n in atoms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
